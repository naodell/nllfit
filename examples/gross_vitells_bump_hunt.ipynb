{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gross-Vitells LEE for a bump hunt\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Consider the case that we are trying to determine whether a reconstructed mass spectrum has evidence of a new resonant state.  That is, given some signal pdf $s(x;\\boldsymbol{\\theta})$ and background pdf $b(x;\\mathbf{a})$ we would like to determine the amplitude, $A$, of the mixture model,\n",
    "\n",
    "$$ \n",
    "f(x;\\mathbf{a}, \\mathbf{\\theta}) = (1 - A)\\,b(x;\\mathbf{a}) + A\\, s(x;\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "The preferred figure of merit in such cases is the *profile likelihood ratio*,\n",
    "\n",
    "$$\n",
    "q_{0} = \\frac{\\mathcal{L}(A, \\hat{\\theta}, \\hat{a})}{\\mathcal{L}(A=0, \\hat{a})}\n",
    "$$\n",
    "\n",
    "where the parameters have been optimized for the data.  This quantity has the nice property that is distributed according to a $\\chi^{2}_{k}$ distribution where in our case the d.o.f. $k$ corresponds to the number of amplitude terms $A$ and is 1.  This is known as [Wilk's Theorem](https://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.E2.80.99_theorem).\n",
    "\n",
    "\n",
    "Frequently, such searches are undertaken with a specific model in mind that can constrain the search parameters, $\\boldsymbol{\\theta}$.  In this notebook I will consider the case that the search parameters are not well constrained, and we would like to adjust the probability to observe a resonance-like fluctuation given it could have appeared anywhere in our search space.\n",
    "\n",
    "Typically, this problem is tackled by carrying out a very large number ($> 10^{6}$) pseudo-experiments which will inevitably be computationally expensive and in some cases infeasible.  The Gross-Vitells approach can be thought of as an extension to Wilk's Therorem where we will factor in the effects of the the unknown search parameters.  This is covered in detail in **citations**.  The important result is that we can represent the excursion probabilities as,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[\\max_{\\theta \\in \\mathcal{M}} q(\\theta) > u] \\approx \\mathbb{E}[\\phi(A_{u})],\n",
    "$$\n",
    "\n",
    "where the excursion set of $q$ for some threshold $u$ is,\n",
    "\n",
    "$$\n",
    "A_{u} = \\{\\theta \\in \\mathcal{M}: q(\\theta) > u\\},\n",
    "$$\n",
    "\n",
    "and $\\phi$ is the [Euler Characteristic](https://en.wikipedia.org/wiki/Euler_characteristic) of the excursion set.  To get some intuition about $\\phi$, it can be thought of as the number of upcrossings of $q$ for some threshold $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy bump hunt\n",
    "\n",
    "To avoid any controversy and control our inputs, I'm going to use as abstracted an example as possible.  To consider the multichannel case we should first start with the case that we have one channel and the data is generated by a combination of an exponentially dropping background continuum and a Voigt profile signal process.\n",
    "\n",
    "$$\n",
    "f_{b} = \\frac{e^{-\\lambda x}}{\\lambda} \\\\\n",
    "f_{s} = V(x; \\mu, \\gamma, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/home/naodell/work/nllfit'\n",
      "/home/naodell/work/projects/nllfit/examples\n"
     ]
    }
   ],
   "source": [
    "# imports and configuration\n",
    "\n",
    "%cd '/home/naodell/work/nllfit'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, chi2\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from nllfit import Parameters, ScanParameters, Model, NLLFitter\n",
    "import nllfit.fit_tools as ft\n",
    "import nllfit.lookee as lee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-59731380bbae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate toy dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mxlimits\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlimits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntoys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bg_model' is not defined"
     ]
    }
   ],
   "source": [
    "# generate toy dataset \n",
    "xlimits  = (200, 2000)\n",
    "sims = ft.generator(bg_model.pdf, xlimits, ntoys=20*n_total)\n",
    "sims = sims.reshape(20, n_total)\n",
    "\n",
    "# plot it\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.hist(data_hgg, bins=40, range=(100, 180), histtype='step')\n",
    "plt.xlabel(r'$\\sf M_{\\gamma\\gamma}$')\n",
    "plt.ylabel('Entries / 2 GeV')\n",
    "plt.xlim(100, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define the background and signal models. For the background, a third-order Legendre polynomial is used.  For the signal, a Gaussian will be used, though a Voigt profile would be more accurate.  Ultimately, both are characteristically the same in that they are fully specified by the location of the central value and the width of the lineshape.  The bounds on $\\mu$ and $\\sigma$ should be set to correspond to the scan range.  In this case the bounds for $\\mu$ are set to be 5 GeV below/above the maximum/minimum values saved in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdf definitions\n",
    "def bg_pdf(x, a, bnds=xlimits):                                                              \n",
    "    '''                                                                        \n",
    "    Wrapper for nth order Legendre Polynomial.            \n",
    "                                                                               \n",
    "    Parameters:                                                                \n",
    "    ===========                                                                \n",
    "    x: data                                                                    \n",
    "    a: model parameters (a1 and a2)                                            \n",
    "    '''      \n",
    "    z   = ft.scale_data(x, xmin=bnds[0], xmax=bnds[1])                                      \n",
    "    fx  = legval(z, a)*2/(bnds[1] - bnds[0])                             \n",
    "    return fx                                                                  \n",
    "\n",
    "def sig_pdf(x, a, normalize=False):                                            \n",
    "    '''                                                                        \n",
    "    Second order Legendre Polynomial (normalized to unity) plus a Gaussian.    \n",
    "                                                                               \n",
    "    Parameters:                                                                \n",
    "    ===========                                                                \n",
    "    x: data                                                                    \n",
    "    a: model parameters (a1, a2, mu, and sigma)                                \n",
    "    '''                                                                        \n",
    "                                                                               \n",
    "    bg = bg_pdf(x, a[3:])                                                     \n",
    "    sig = norm.pdf(x, a[1], a[2])                                              \n",
    "    if normalize:                                                              \n",
    "        sig_norm = integrate.quad(lambda z: norm.pdf(z, a[1], a[2]), -1, 1)[0] \n",
    "    else:                                                                      \n",
    "        sig_norm = 1.                                                          \n",
    "                                                                               \n",
    "    return (1 - a[0])*bg + a[0]*sig/sig_norm                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "bg_params = Parameters()\n",
    "bg_params.add_many(\n",
    "                   ('a0', 0.5, False, None, None, None),\n",
    "                   ('a1', 0., True, None, None, None),\n",
    "                   ('a2', 0., True, None, None, None),\n",
    "                   ('a3', 0., True, None, None, None)\n",
    "                  )\n",
    "\n",
    "bg_model  = Model(bg_pdf, bg_params)\n",
    "bg_fitter = NLLFitter(bg_model, verbose=True)\n",
    "bg_result = bg_fitter.fit(data_hgg, calculate_corr=True)\n",
    "\n",
    "# print 'bg result'\n",
    "# bg_params.pretty_print()\n",
    "# print '\\n'\n",
    "\n",
    "sig_params = Parameters()\n",
    "sig_params.add_many(\n",
    "                    ('A'     , 0.01 , True , 0.0  , 1.  , None) ,\n",
    "                    ('mu'    , 125.  , True , 120. , 130. , None) ,\n",
    "                    ('sigma' , 1.   , True , 0.45 , 10.  , None)\n",
    "                   )\n",
    "sig_params += bg_params.copy()\n",
    "sig_model  = Model(sig_pdf, sig_params)\n",
    "sig_fitter = NLLFitter(sig_model, verbose=True)\n",
    "sig_result = sig_fitter.fit(data_hgg, calculate_corr=True)\n",
    "\n",
    "# print 'bg+signal result'\n",
    "# sig_params.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.fit_plot_1D(data_hgg, sig_model, bg_model, xlimits, nbins=40, suffix='hgg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the signal model is the mixture of the background polynomial and the Gaussian.  In both cases, the models are fit to the data so that we can use the parameters for generating MC.  It will be important to know the value of the likelihood ratio so that we can later determine the corresponding global p value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmax = 2*(bg_model.calc_nll(data_hgg) - sig_model.calc_nll(data_hgg))\n",
    "qmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the coefficients for the Gross-Vitells treatment, the search parameters will be scanned over a number of pseudo-datasets generated from the background-only model.  This is done by using a [simple acceptance-rejection sampler](https://github.com/naodell/amumu/blob/master/nllfitter/fit_tools.py#L146).  The number of toys necessary to get a good estimate is typically on the order of ten, but I will generate a 1000 for the sake of validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate simulated data\n",
    "sims = ft.generator(bg_model.pdf, xlimits, ntoys=20*n_total)\n",
    "sims = sims.reshape(20, n_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the mass of the resonance ($\\mu$) and its width ($\\sigma$) need to be scanned over.  For the mass, we will restrict the range under consideration to masses of 110 GeV to 170 GeV (in practice, the range that is scanned is at the discretion of the analyzer and should be a range of values where you would be willing to accept the presence of an anomaly, i.e., masses that are not a priori excluded).  The range of widths should be treated similarily; a reasonable value for the lower bound should be your experimental resolution and the upper bound should be chosen based on the physics models under consideration.  \n",
    "\n",
    "The `ScanParameters` class will generate bounded ranges for the parameters under consideration.  The parameters will be allowed to vary within those bounds and take on the values that minimize the NLL.  Choosing the appropriate granularity is up to the discretion of the analyzer.  I would suggest not making the ranges too granular since you will spend a lot more time running the scans with no benefit.  Also, when doing this it makes sense to first consider the effect of one dimensional scans before undertaking higher dimensional scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define scan parameters\n",
    "sigma_max = sig_params['sigma'].value\n",
    "nscans = [30, 10]\n",
    "# bnds   = [(xlimits[0]+10, xlimits[1]-10), (sigma_max, sigma_max)]\n",
    "bnds   = [(xlimits[0]+10, xlimits[1]-10), (1, 5)]\n",
    "\n",
    "scan_params = ScanParameters(names  = ['mu', 'sigma'],\n",
    "                             bounds = bnds,\n",
    "                             nscans = nscans\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to carry out the scans.  For each toy dataset, we will save the parameters that minimize the nll, the value of the EC for several excursion thresholds `u_0`.  I should mention that I used [Kyle Cranmer's notebook](https://github.com/cranmer/look-elsewhere-2d) on GV as a starting point.  For the most part, my framework is completely independent, but I did take his [code for calculating the EC](https://github.com/cranmer/look-elsewhere-2d).  It makes very nice use of numpy's image convolution modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# carry out scans of signal parameters and cacluate EC for varying excursions\n",
    "\n",
    "bg_fitter.verbose = False # turn off parameter printing\n",
    "phiscan   = []\n",
    "qmaxscan  = []\n",
    "u_0       = np.linspace(0.2, 30., 149)\n",
    "for i, sim in tqdm_notebook(enumerate(sims), \n",
    "                            desc='Scanning', \n",
    "                            unit_scale=True, \n",
    "                            ncols=75, \n",
    "                            total=len(sims)):\n",
    "    # fit background model\n",
    "    bg_result = bg_fitter.fit(sim, calculate_corr=False)\n",
    "    if bg_result.status == 0:\n",
    "        nll_bg = bg_model.calc_nll(sim)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    # scan over signal parameters\n",
    "    nllscan, params, dof = sig_fitter.scan(scan_params, sim)\n",
    "    qscan = -2*(nllscan - nll_bg)\n",
    "    qmaxscan.append(np.max(qscan))\n",
    "\n",
    "    ### Calculate E.C. of the random field\n",
    "    if qscan.size != np.prod(scan_params.nscans):\n",
    "        continue\n",
    "        \n",
    "    qscan = np.array(qscan).reshape(scan_params.nscans)\n",
    "    phiscan.append([lee.calculate_euler_characteristic((qscan > u) + 0.) \n",
    "                    for u in u_0])\n",
    "\n",
    "# convert lists to arrays\n",
    "phiscan     = np.array(phiscan)\n",
    "qmaxscan    = np.array(qmaxscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the scans and calculated the corresponding EC, the coefficients for the EC densities can be determined.  This is done by carrying out a fit to the scan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "param_init = [1.,1.]\n",
    "param_bnds = [(1., np.inf), (1., np.inf)]\n",
    "\n",
    "kvals      = [1]\n",
    "scales     = [0.5]\n",
    "nvals      = lee.get_GV_coefficients(u_0, phiscan, param_init, param_bnds, kvals, scales)\n",
    "nvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, calculate the relevant statistics and print the results,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "### Calculate statistics and print ###\n",
    "p_local  = 0.5*chi2.sf(qmax, 1)\n",
    "z_local  = -norm.ppf(p_local)\n",
    "p_global = lee.get_p_global(qmax, [1], [nvals], [0.5])\n",
    "z_global = -norm.ppf(p_global)\n",
    "\n",
    "for i, n in enumerate(nvals.flatten()):\n",
    "    print 'N{0} = {1:.2f}'.format(i+1, n)\n",
    "\n",
    "print 'local p value       = {0:.3e}'.format(p_local)\n",
    "print 'local significance  = {0:.2f}'.format(z_local)\n",
    "print 'global p value      = {0:.3e}'.format(p_global)\n",
    "print 'global significance = {0:.2f}'.format(z_global)\n",
    "print 'trial factor        = {0:.2f}'.format(p_global/p_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of validating this result we can check if our prediction from the Gross-Vitells formalism compares well with the distribution from the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lee.gv_validation_plot(u_0, phiscan, qmaxscan, [nvals], [1], [0.5], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higgs to ZZ to 4 leptons analysis\n",
    "\n",
    "Now we will consider the other golden decay channel: $H\\rightarrow ZZ \\rightarrow 4\\ell$.  This channel is distinct from the $H\\rightarrow \\gamma\\gamma$ channel in specifics (background shape, signal shape, and overall event yield), but when it comes to hypothesis testing the two channels should be combined to determine the total significance.\n",
    "\n",
    "Let's go through the same steps that we went through for the $H\\rightarrow \\gamma\\gamma$ case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/toy_hzz.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c0606f61aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the data and plot it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/toy_hzz.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_hzz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_mass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_total\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata_hzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naodell/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naodell/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naodell/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naodell/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naodell/opt/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/toy_hzz.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# get the data and plot it\n",
    "df_data  = pd.read_csv('data/toy_hzz.txt')\n",
    "data_hzz = df_data.inv_mass.values\n",
    "n_total  = data_hzz.size\n",
    "\n",
    "df_bg       = pd.read_csv('data/toy_hzz_bg.txt')\n",
    "data_hzz_bg = df_bg.inv_mass.values[:7500]\n",
    "\n",
    "plt.hist(data_hzz, bins=27, range=(100, 181), histtype='step')\n",
    "plt.hist(data_hzz_bg, bins=27, range=(100, 181), histtype='step', weights=0.01*np.ones(data_hzz_bg.size))\n",
    "plt.xlabel(r'$\\sf M_{ZZ}$')\n",
    "plt.ylabel('Entries / 2 GeV')\n",
    "plt.xlim(100, 180)\n",
    "plt.ylim(0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_fitter.verbose = True\n",
    "bg_result  = bg_fitter.fit(data_hzz_bg)\n",
    "bg_params['a1'].vary   = False\n",
    "bg_params['a2'].vary   = False\n",
    "bg_params['a3'].vary   = False\n",
    "\n",
    "# fix bg+signal polynomial parameters to values from bg only simulation\n",
    "sig_params['a1'].value  = bg_params['a1'].value\n",
    "sig_params['a2'].value  = bg_params['a2'].value\n",
    "sig_params['a3'].value  = bg_params['a3'].value\n",
    "sig_params['a1'].vary   = False\n",
    "sig_params['a2'].vary   = False\n",
    "sig_params['a3'].vary   = False\n",
    "\n",
    "sig_result = sig_fitter.fit(data_hzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.fit_plot_1D(data_hzz, sig_model, bg_model, (100, 181), nbins=27, suffix='hzz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate toys\n",
    "sims = ft.generator(bg_model.pdf, xlimits, ntoys=500*n_total)\n",
    "sims = sims.reshape(500, n_total)\n",
    "\n",
    "# set up scan parameters\n",
    "sigma_max = sig_params['sigma'].value\n",
    "nscans = [30, 10]\n",
    "bnds   = [(xlimits[0]+10, xlimits[1]-10), (1, 5)]\n",
    "scan_params = ScanParameters(names  = ['mu', 'sigma'],\n",
    "                             bounds = bnds,\n",
    "                             nscans = nscans\n",
    "                            )\n",
    "\n",
    "# carry out scans over signal parameters\n",
    "bg_fitter.verbose = False # turn off parameter printing\n",
    "phiscan   = []\n",
    "qmaxscan  = []\n",
    "u_0       = np.linspace(0.01, 30., 300)\n",
    "for i, sim in tqdm_notebook(enumerate(sims), \n",
    "                            desc='Scanning', \n",
    "                            unit_scale=True, \n",
    "                            ncols=75, \n",
    "                            total=len(sims)):\n",
    "    # fit background model\n",
    "    bg_result = bg_fitter.fit(sim, calculate_corr=False)\n",
    "    if bg_result.status == 0:\n",
    "        nll_bg = bg_model.calc_nll(sim)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    # scan over signal parameters\n",
    "    nllscan, params, dof = sig_fitter.scan(scan_params, sim)\n",
    "    qscan = -2*(nllscan - nll_bg)\n",
    "    qmaxscan.append(np.max(qscan))\n",
    "\n",
    "    ### Calculate E.C. of the random field\n",
    "    if qscan.size != np.prod(scan_params.nscans):\n",
    "        continue\n",
    "        \n",
    "    qscan = np.array(qscan).reshape(scan_params.nscans)\n",
    "    phiscan.append([lee.calculate_euler_characteristic((qscan > u) + 0.) \n",
    "                    for u in u_0])\n",
    "\n",
    "# convert lists to arrays\n",
    "phiscan     = np.array(phiscan)\n",
    "qmaxscan    = np.array(qmaxscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_init = [1.,1.]\n",
    "param_bnds = [(0., np.inf), (0., np.inf)]\n",
    "kvals      = [1]\n",
    "scales     = [0.5]\n",
    "nvals      = lee.get_GV_coefficients(u_0, phiscan, param_init, param_bnds, kvals, scales)\n",
    "\n",
    "### Calculate statistics and print ###\n",
    "p_local  = 0.5*chi2.sf(qmax, 1)\n",
    "z_local  = -norm.ppf(p_local)\n",
    "p_global = lee.get_p_global(qmax, [1], [nvals], [0.5])\n",
    "z_global = -norm.ppf(p_global)\n",
    "\n",
    "for i, n in enumerate(nvals.flatten()):\n",
    "    print 'N{0} = {1:.2f}'.format(i+1, n)\n",
    "\n",
    "print 'local p value       = {0:.3e}'.format(p_local)\n",
    "print 'local significance  = {0:.2f}'.format(z_local)\n",
    "print 'global p value      = {0:.3e}'.format(p_global)\n",
    "print 'global significance = {0:.2f}'.format(z_global)\n",
    "print 'trial factor        = {0:.2f}'.format(p_global/p_local)\n",
    "\n",
    "lee.gv_validation_plot(u_0, phiscan, qmaxscan, [nvals], [1], [0.5], None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very good in both cases.  I will consider the case that we carry out the search in both channels simultaneously in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "67px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "09fe3b0c70a1434695b0603e2ed3e8f7": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "22f1ed9d68bc4ff7aae42ba2d1cb1788": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
